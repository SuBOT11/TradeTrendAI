{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfe import df_selected\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_selected.drop(columns=['Close'])\n",
    "y = df_selected['Close']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_array = X_train.values\n",
    "X_test_array= X_test.values\n",
    "y_train_array= y_train.values\n",
    "y_test_array = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_array, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_array, dtype=torch.float32)\n",
    "\n",
    "# Convert test NumPy arrays to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch - Features shape: torch.Size([3, 10])\n",
      "Training batch - Target shape: torch.Size([3])\n",
      "Test batch - Features shape: torch.Size([3, 10])\n",
      "Test batch - Target shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "X_train_batch, y_train_batch = next(iter(train_loader))\n",
    "\n",
    "# Load a batch of data from test_loader\n",
    "X_test_batch, y_test_batch = next(iter(test_loader))\n",
    "print(\"Training batch - Features shape:\", X_train_batch.shape)\n",
    "print(\"Training batch - Target shape:\", y_train_batch.shape)\n",
    "print(\"Test batch - Features shape:\", X_test_batch.shape)\n",
    "print(\"Test batch - Target shape:\", y_test_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "      batch_size = x.size(0)\n",
    "      h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).to(x.device)\n",
    "      c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).to(x.device)\n",
    "\n",
    "      # Ensure input has the correct shape\n",
    "      x = x.unsqueeze(1)  # Add a singleton dimension for the sequence length\n",
    "\n",
    "      _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "      out = self.linear(hn[-1]).squeeze()\n",
    "\n",
    "      return out\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "num_hidden_units = 8\n",
    "\n",
    "# Initialize the model\n",
    "model = ShallowRegressionLSTM(input_size=10, hidden_units=num_hidden_units)\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "--------\n",
      "Test loss: 182676.26132150425\n",
      "\n",
      "Epoch 0\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsl_jansu/anaconda3/envs/data_prep/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 202940.2069313033\n",
      "Test loss: 182255.7177899894\n",
      "\n",
      "Epoch 1\n",
      "---------\n",
      "Train loss: 202486.65194396322\n",
      "Test loss: 181828.393819518\n",
      "\n",
      "Epoch 2\n",
      "---------\n",
      "Train loss: 202033.51699093817\n",
      "Test loss: 181405.54834811972\n",
      "\n",
      "Epoch 3\n",
      "---------\n",
      "Train loss: 201581.35600179905\n",
      "Test loss: 180982.58481197033\n",
      "\n",
      "Epoch 4\n",
      "---------\n",
      "Train loss: 201129.82877048908\n",
      "Test loss: 180560.48809917903\n",
      "\n",
      "Epoch 5\n",
      "---------\n",
      "Train loss: 200680.07486007462\n",
      "Test loss: 180140.39979475635\n",
      "\n",
      "Epoch 6\n",
      "---------\n",
      "Train loss: 200231.7319013193\n",
      "Test loss: 179720.92919094278\n",
      "\n",
      "Epoch 7\n",
      "---------\n",
      "Train loss: 199784.11470965485\n",
      "Test loss: 179302.27289128708\n",
      "\n",
      "Epoch 8\n",
      "---------\n",
      "Train loss: 199336.73258012393\n",
      "Test loss: 178883.14017809852\n",
      "\n",
      "Epoch 9\n",
      "---------\n",
      "Train loss: 198888.68701692432\n",
      "Test loss: 178463.56586003708\n",
      "\n",
      "Epoch 10\n",
      "---------\n",
      "Train loss: 198440.85835054636\n",
      "Test loss: 178044.83754303496\n",
      "\n",
      "Epoch 11\n",
      "---------\n",
      "Train loss: 197993.99447378065\n",
      "Test loss: 177626.0009600106\n",
      "\n",
      "Epoch 12\n",
      "---------\n",
      "Train loss: 197547.62949760127\n",
      "Test loss: 177208.3730799788\n",
      "\n",
      "Epoch 13\n",
      "---------\n",
      "Train loss: 197101.48615738272\n",
      "Test loss: 176792.57916777013\n",
      "\n",
      "Epoch 14\n",
      "---------\n",
      "Train loss: 196656.50619253065\n",
      "Test loss: 176376.3391651218\n",
      "\n",
      "Epoch 15\n",
      "---------\n",
      "Train loss: 196212.07967833822\n",
      "Test loss: 175960.7861493644\n",
      "\n",
      "Epoch 16\n",
      "---------\n",
      "Train loss: 195768.8739130797\n",
      "Test loss: 175545.05844478283\n",
      "\n",
      "Epoch 17\n",
      "---------\n",
      "Train loss: 195325.6881288313\n",
      "Test loss: 175131.35407838982\n",
      "\n",
      "Epoch 18\n",
      "---------\n",
      "Train loss: 194883.35315748266\n",
      "Test loss: 174717.94913599046\n",
      "\n",
      "Epoch 19\n",
      "---------\n",
      "Train loss: 194442.26198943896\n",
      "Test loss: 174305.8933064089\n",
      "\n",
      "Epoch 20\n",
      "---------\n",
      "Train loss: 194001.57772937766\n",
      "Test loss: 173895.02206369175\n",
      "\n",
      "Epoch 21\n",
      "---------\n",
      "Train loss: 193562.55252615272\n",
      "Test loss: 173483.67834679555\n",
      "\n",
      "Epoch 22\n",
      "---------\n",
      "Train loss: 193123.96123317565\n",
      "Test loss: 173074.19200211865\n",
      "\n",
      "Epoch 23\n",
      "---------\n",
      "Train loss: 192686.3043876599\n",
      "Test loss: 172665.12840969278\n",
      "\n",
      "Epoch 24\n",
      "---------\n",
      "Train loss: 192248.93353961222\n",
      "Test loss: 172255.5741856462\n",
      "\n",
      "Epoch 25\n",
      "---------\n",
      "Train loss: 191810.60857126198\n",
      "Test loss: 171846.7653767214\n",
      "\n",
      "Epoch 26\n",
      "---------\n",
      "Train loss: 191374.1139475613\n",
      "Test loss: 171439.9133673199\n",
      "\n",
      "Epoch 27\n",
      "---------\n",
      "Train loss: 190938.05195562367\n",
      "Test loss: 171032.1113115731\n",
      "\n",
      "Epoch 28\n",
      "---------\n",
      "Train loss: 190502.4200509728\n",
      "Test loss: 170625.27966101695\n",
      "\n",
      "Epoch 29\n",
      "---------\n",
      "Train loss: 190067.55104777453\n",
      "Test loss: 170217.3571239407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(train_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "\n",
    "def test_model(test_loader, model, loss_function):\n",
    "\n",
    "    num_batches = len(test_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "print(\"Untrained test\\n--------\")\n",
    "test_model(test_loader, model, loss_function)\n",
    "\n",
    "for ix_epoch in range(30):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    test_model(test_loader, model, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 11.7846, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([11.2422,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 12.1778]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 11.7846, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 14.2255]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 12.1778]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 16.8160, 11.7846]), tensor([16.8160, 11.2422,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.7846, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([11.2422, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 11.2422]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([11.7846, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([11.2422, 16.8160, 11.2422]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([11.2422, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840])]\n",
      "[tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 11.2422, 16.8160]), tensor([16.8160,  9.1840, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.7846]), tensor([ 9.1840, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.7846, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 14.2255, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 11.2422, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 14.2255, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160,  9.1840]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([ 9.1840, 11.7846, 16.8160]), tensor([16.8160,  9.1840, 16.8160]), tensor([11.2422, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 11.2422]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor([16.8160, 16.8160, 16.8160]), tensor(11.2422)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 117) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_data\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Call the function and check the results\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_prediction_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined data shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, combined_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[62], line 26\u001b[0m, in \u001b[0;36mcreate_prediction_dataframe\u001b[0;34m(train_loader, test_loader, model, target_column)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_prediction_dataframe\u001b[39m(train_loader, test_loader, model, target_column):\n\u001b[1;32m     25\u001b[0m     train_predictions \u001b[38;5;241m=\u001b[39m generate_predictions(train_loader, model, target_column)\n\u001b[0;32m---> 26\u001b[0m     test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain predictions shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_predictions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest predictions shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_predictions\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[62], line 17\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[0;34m(data_loader, model, target_column)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions:  \u001b[38;5;66;03m# Check if predictions list is not empty\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[0;32m---> 17\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])  \u001b[38;5;66;03m# Handle case where no predictions were made\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 117) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def generate_predictions(data_loader, model, target_column):\n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_pred = model(X)\n",
    "            if y_pred.numel() > 0:  # Check if y_pred is not empty\n",
    "                predictions.append(y_pred)\n",
    "    \n",
    "    if predictions:  # Check if predictions list is not empty\n",
    "        print(predictions)\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "    else:\n",
    "        predictions = torch.tensor([])  # Handle case where no predictions were made\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_prediction_dataframe(train_loader, test_loader, model, target_column):\n",
    "    train_predictions = generate_predictions(train_loader, model, target_column)\n",
    "    test_predictions = generate_predictions(test_loader, model, target_column)\n",
    "    \n",
    "    print(\"Train predictions shape:\", train_predictions.shape)\n",
    "    print(\"Test predictions shape:\", test_predictions.shape)\n",
    "    \n",
    "    train_data = train_loader.dataset.data\n",
    "    test_data = test_loader.dataset.data\n",
    "    \n",
    "    print(\"Train data shape:\", train_data.shape)\n",
    "    print(\"Test data shape:\", test_data.shape)\n",
    "    \n",
    "    train_data[target_column + \"_predicted\"] = train_predictions.numpy()\n",
    "    test_data[target_column + \"_predicted\"] = test_predictions.numpy()\n",
    "    \n",
    "    combined_data = pd.concat([train_data, test_data], axis=0)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Call the function and check the results\n",
    "combined_data = create_prediction_dataframe(train_loader, test_loader, model, 'pred_value')\n",
    "print(\"Combined data shape:\", combined_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
